{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Parsing: EAD XML Finding Aids from the Library of Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a directory called 'LOC_Metadata' and setting it as our current working directory\n",
    "\n",
    "!mkdir /sharedfolder/LOC_Metadata\n",
    "\n",
    "os.chdir('/sharedfolder/LOC_Metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To make this notebook self-contained, we'll download a list of XML finding aid files the 'right' way.\n",
    "##  (In practice I normally use the 'find-and-replace + grep + wget' approach we covered in class,\n",
    "##   because it takes some extra effort to remind myself how to parse the HTML page via BeautifulSoup.)\n",
    "\n",
    "## We first load a page with links to finding aids in the 'recorded sound' collection.\n",
    "\n",
    "finding_aid_list_url = 'http://findingaids.loc.gov/source/RS'\n",
    "\n",
    "finding_aid_list_page = urlopen(finding_aid_list_url).read().decode('utf8')  # Loading the page\n",
    "\n",
    "print(finding_aid_list_page[:700])  # Printing the first 700 characters in the page we just loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we'll parse the page's HTML using BeautifulSoup ...\n",
    "\n",
    "soup = BeautifulSoup(finding_aid_list_page, 'lxml')\n",
    "\n",
    "## ... and examine soup.find_all('a'), which returns a list of 'a' elements (i.e., HTML links).\n",
    "\n",
    "print(len(soup.find_all('a')))         # Checking the number of links on the page\n",
    "\n",
    "print()                                # Printing a blank line for readability\n",
    "\n",
    "print(soup.find_all('a')[70])          # Printing element #70 in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can access the 'href' attribute of an element (i.e., the link URL) using 'href' in \n",
    "## brackets, just like a dictionary.\n",
    "\n",
    "soup.find_all('a')[70]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's make a list of every link on the page.\n",
    "\n",
    "all_links = []\n",
    "\n",
    "for element in soup.find_all('a'):         # Looping through all 'a' elements.\n",
    "    try:                                   # Because some 'a' elements do not contain 'href' attributes, \n",
    "        all_links.append(element['href'])  ## we can use a try/except statement to skip elements that \n",
    "    except:                                ## would otherwise raise an error.\n",
    "        pass\n",
    "\n",
    "all_links[:15]                             # Outputting the first 15 links in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We know that the URL for every XML file we're looking for ends in '.2', so we can\n",
    "## use that fact to filter out irrelevant links.\n",
    "\n",
    "xml_urls = []\n",
    "\n",
    "for link in all_links:\n",
    "    if link[-2:] == '.2':           # Checking whether the last two characters of a link are '.2'\n",
    "        xml_urls.append(link)\n",
    "\n",
    "xml_urls                            # Outputting the full list of relevant XML URLs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading each XML file in our list of URLs\n",
    "\n",
    "## We can use the subprocess module (which we imported above) to issue commands in the bash shell.\n",
    "## In an interactive bash shell session we'd use spaces to separate arguments; instead, subprocess\n",
    "## takes arguments in the form of a Python list.\n",
    "\n",
    "## For each item in our list, the following issues a command with two arguments: 'wget' followed by the URL.\n",
    "## It thus downloads each XML file to the current directory.\n",
    "\n",
    "for url in xml_urls:\n",
    "    subprocess.call(['wget', url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outputting a list of filenames in the current directory\n",
    "\n",
    "## In Unix-like operating systems, './' always refers to the current directory.\n",
    "\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just in case there are other files in the current directory, we can use a \n",
    "## list comprehension to create a list of filenames that end in '.2' and assign\n",
    "## it to the variable 'xml_filenames'.\n",
    "\n",
    "xml_filenames = [item for item in os.listdir('./') if item[-2:]=='.2']\n",
    "\n",
    "xml_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's choose an arbitrary XML file in our collection so we can figure out how to parse it.\n",
    "\n",
    "xml_filename = xml_filenames[4]       ## Selecting filename #4 in our list\n",
    "\n",
    "xml_text = open(xml_filename).read()  ## Reading the file and assigning its content to the variable 'xml_text'\n",
    "\n",
    "print(xml_text[:700])                 ## Printing the first 700 characters in the XML text we just loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse the XML text from the previous cell using Beautiful Soup\n",
    "\n",
    "soup = BeautifulSoup(xml_text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By looking at the XML text above, we can see that the 'ead' element is the root of our XML tree.\n",
    "## Let's use a for loop to look at the names of elements one next level down in the tree.\n",
    "\n",
    "for element in soup.ead:\n",
    "    print(element.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In practice you'd usually just look through the XML file by eye, identify the elements \n",
    "## you're looking for, and use soup.find_all('...') to extract them. For now, let's continue \n",
    "## working down the XML tree with BeautifulSoup.\n",
    "\n",
    "# You can find a glossary of EAD element names here:\n",
    "# https://loc.gov/ead/EAD3taglib/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the 'eadheader' element is administrative metadata we don't care about, let's \n",
    "## repeat the process for 'soup.ead.archdesc' ('archdesc' is 'archival description' in EAD parlance).\n",
    "\n",
    "for element in soup.ead.archdesc:\n",
    "    if element.name != None:  ## Filtering out 'None' elements, which in this case are irrelevant comments\n",
    "        print(element.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By looking at the XML file in a text editor, I notice the 'did' element ('descriptive identification')\n",
    "## contains the item-level information we're looking for. Let's run another for loop to look at the \n",
    "## names of elements contained within each 'did' element.\n",
    "\n",
    "for element in soup.ead.archdesc.did:\n",
    "    if element.name != None:\n",
    "        print(element.name)\n",
    "\n",
    "## Note that 'soup.ead.archdesc.did' only refers to the first 'did' element in the XML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OK, that's enough exploring. Let's use soup.find_all() to create a list of 'did' elements. \n",
    "\n",
    "did_elements = soup.find_all('did')\n",
    "\n",
    "print(len(did_elements))           ## Printing the number of 'did' elements in our list\n",
    "\n",
    "print()\n",
    "\n",
    "print(did_elements[4])             ## Printing item #4 in the the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not every 'did' element contains the same fields; different objects are described differently.\n",
    "\n",
    "## Try running this cell several times, plugging in other index numbers to compare the way\n",
    "## different items' records are formatted.\n",
    "\n",
    "print(did_elements[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you run the cell above several times with different index numbers, you'll notice that the \n",
    "## first item in the list (index 0) refers to the entire box of records, while the others are \n",
    "## individual folders or series of folders.\n",
    "\n",
    "## To make things more complicated, some items are physically described using 'container' elements \n",
    "## while others use 'extent' instead. Most appear to include 'unittitle' and 'unitdate'.\n",
    "\n",
    "## Our goal is to create a CSV that contains a basic description of each 'unit', or 'did' element,\n",
    "## in each XML finding aid. For the purposes of this exercise, let's include the following pieces \n",
    "## of information for each unit, where available:\n",
    "\n",
    "#### title of the source collection\n",
    "#### unittitle\n",
    "#### unitdate\n",
    "#### container type\n",
    "#### container number\n",
    "#### extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since each XML finding aid represents a single collection, we'll want to include a column that \n",
    "## identifies which collection it comes from. By reading through the XML files, we see that each \n",
    "## has a single element called 'titleproper' that describes the whole collection.\n",
    "\n",
    "## Let's create a recipe to extract that text. Here's a first try:\n",
    "\n",
    "collection_title = soup.find('titleproper').get_text()\n",
    "\n",
    "collection_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## That format is OK, but we should remove the tab and newline characters. Let's try again, using \n",
    "## the replace() function to replace them with spaces.\n",
    "\n",
    "collection_title = soup.find('titleproper').get_text().replace('\\t', ' ').replace('\\n', ' ')\n",
    "\n",
    "collection_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can add the strip() function to remove the space at the end of the string.\n",
    "\n",
    "collection_title = soup.find('titleproper').get_text().replace('\\t', ' ').replace('\\n', ' ').strip()\n",
    "\n",
    "collection_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We still have a series of spaces in a row in the middle of the string. We can use a 'while loop' \n",
    "## to repeatedly replace any occurrence of '  ' (two spaces) with ' ' (one space).\n",
    "\n",
    "collection_title = soup.find('titleproper').get_text().replace('\\t', ' ').replace('\\n', ' ').strip()\n",
    "\n",
    "while '  ' in collection_title:\n",
    "    collection_title = collection_title.replace('  ', ' ')\n",
    "\n",
    "collection_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perfect. We'll extract the collection name whenever we open an XML finding aid and include it \n",
    "## in each CSV row associated with that collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now on to 'unittitle'. Recall that we created a list of 'did' elements above, called 'did_elements'.\n",
    "\n",
    "element = did_elements[4]\n",
    "\n",
    "unittitle = element.find('unittitle').get_text()\n",
    "\n",
    "unittitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since those tabs and newlines are a recurring probem, we should define a function that \n",
    "## removes them from any given text string.\n",
    "\n",
    "def clean_text(text):\n",
    "    temp_text = text.replace('\\t', ' ').replace('\\n', ' ').strip()\n",
    "    while '  ' in temp_text:\n",
    "        temp_text = temp_text.replace('  ', ' ')\n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our clean_text() function.\n",
    "\n",
    "element = did_elements[4]\n",
    "\n",
    "unittitle = element.find('unittitle').get_text()\n",
    "\n",
    "unittitle = clean_text(unittitle)\n",
    "\n",
    "unittitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's try extracting the 'unittitle' field for each 'did' element in our list.\n",
    "\n",
    "for element in did_elements:\n",
    "    unittitle = element.get_text().replace('\\t', ' ').replace('\\n', ' ').strip()\n",
    "    print(clean_text(unittitle))\n",
    "    print('-----------------')       # Printing a divider between elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first element in the list above contains more information than we need, but we can\n",
    "## let that slide for this exercise.\n",
    "\n",
    "## Next is 'unitdate'. We'll use our clean_text() function once again.\n",
    "\n",
    "element = did_elements[4]\n",
    "\n",
    "unitdate = element.find('unitdate').get_text()\n",
    "\n",
    "unitdate = clean_text(unitdate)\n",
    "\n",
    "unitdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's loop through the list of 'did' elements and see if our 'unittitle' recipe holds up.\n",
    "\n",
    "for element in did_elements:\n",
    "    unitdate = element.find('unitdate').get_text()\n",
    "    print(clean_text(unitdate))\n",
    "    print('-----------------')       # Printing a divider between elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now on to container type and number. Let's examine a 'container' XML element.\n",
    "\n",
    "element = did_elements[4]\n",
    "\n",
    "element.find('container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the container type ('folder', in this case) is an attribute in the 'container' tag, \n",
    "## we can extract it using bracket notation.\n",
    "\n",
    "element = did_elements[4]\n",
    "\n",
    "container_type = element.find('container')['type']\n",
    "\n",
    "container_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The container number is specified between the opening and closing 'container' tags, \n",
    "## so we can get it using get_text().\n",
    "\n",
    "element = did_elements[4]\n",
    "\n",
    "container_number = element.find('container').get_text()\n",
    "\n",
    "container_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next we'll try to get the container type and number for each 'did' element in our list ...\n",
    "\n",
    "for element in did_elements:\n",
    "    container_type = element.find('container')['type']\n",
    "    print(container_type)\n",
    "\n",
    "    container_number = element.find('container').get_text()\n",
    "    print(container_number)\n",
    "\n",
    "    print('-----------------')       # Printing a divider between elements\n",
    "\n",
    "## ... and we get an error. The reason is that some 'did' elements don't include a 'container' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using try/accept notation, whenever we get an error because a container element isn't found,\n",
    "## we can revert to '' (an empty string) instead.\n",
    "\n",
    "for element in did_elements:\n",
    "    try:\n",
    "        container_type = element.find('container')['type']\n",
    "    except:\n",
    "        container_type = ''\n",
    "    print(container_type)\n",
    "    \n",
    "    try:\n",
    "        container_number = element.find('container').get_text()\n",
    "    except:\n",
    "        container_number = ''\n",
    "    print(container_number)\n",
    "    print('-----------------')       # Printing a divider between elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The last field we'll extract is 'extent', which is only included in a handful of 'did' elements.\n",
    "\n",
    "element = did_elements[3]\n",
    "\n",
    "extent = element.find('extent').get_text()\n",
    "\n",
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's extract 'extent' from each element in our list of 'did' elements (for those that happen to include it).\n",
    "\n",
    "for element in did_elements:\n",
    "    try:\n",
    "        extent = element.find('extent').get_text()\n",
    "    except:\n",
    "        extent = ''\n",
    "    print(extent)\n",
    "    print('-----------------')       # Printing a divider between elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's put it all together and view our chosen fields for a single 'did' element.\n",
    "## We will combine our fields in a list to create a 'row' for our future CSV file.\n",
    "\n",
    "element = did_elements[6]\n",
    "\n",
    "# unittitle\n",
    "try:                           # Added try/except statements for 'unittitle' and 'unitdate' just to be safe\n",
    "    unittitle = clean_text(element.find('unittitle').get_text())\n",
    "except:\n",
    "    unittitle = ''\n",
    "    \n",
    "# unitdate\n",
    "try:\n",
    "    unitdate = clean_text(element.find('unitdate').get_text())\n",
    "except:\n",
    "    unitdate = ''\n",
    "    \n",
    "# container type and number\n",
    "try:\n",
    "    container_type = element.find('container')['type']\n",
    "except:\n",
    "    container_type = ''\n",
    "\n",
    "try:\n",
    "    container_number = element.find('container').get_text()\n",
    "except:\n",
    "    container_number = ''\n",
    "\n",
    "# extent\n",
    "try:\n",
    "    extent = element.find('extent').get_text()\n",
    "except:\n",
    "    extent = ''\n",
    "\n",
    "row = [unittitle, unitdate, container_type, container_number, extent]\n",
    "\n",
    "\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's take a step back and generalize, so that we can extract metadata for each \n",
    "## 'did' element in a single XML file.\n",
    "\n",
    "## We will also include the 'collection title' field ('titleproper' in EAD's vocabulary) as \n",
    "## the first item in each row.\n",
    "\n",
    "xml_filename = xml_filenames[3] # <-- Change the index number there to run the script on another XML file in the list.\n",
    "\n",
    "\n",
    "xml_text = open(xml_filename).read()\n",
    "\n",
    "soup = BeautifulSoup(xml_text, 'lxml')\n",
    "\n",
    "list_of_lists = []   # Creating an empty list, which we will use to hold our rows (each row represented as a list)\n",
    "\n",
    "\n",
    "try:\n",
    "    collection_title = clean_text(soup.find('titleproper').get_text())\n",
    "except:\n",
    "    collection_title = xml_filename   # If the 'titleproper' field is missing for some reason,\n",
    "                                      ## we'll use the XML filename instead.\n",
    "\n",
    "for element in soup.find_all('did'):\n",
    "\n",
    "    # unittitle\n",
    "    try:\n",
    "        unittitle = clean_text(element.find('unittitle').get_text())\n",
    "    except:\n",
    "        unittitle = ''\n",
    "    \n",
    "    # unitdate\n",
    "    try:\n",
    "        unitdate = clean_text(element.find('unitdate').get_text())\n",
    "    except:\n",
    "        unitdate = ''\n",
    "    \n",
    "    # container type and number\n",
    "    try:\n",
    "        container_type = element.find('container')['type']\n",
    "    except:\n",
    "        container_type = ''\n",
    "\n",
    "    try:\n",
    "        container_number = element.find('container').get_text()\n",
    "    except:\n",
    "        container_number = ''\n",
    "\n",
    "    # extent\n",
    "    try:\n",
    "        extent = element.find('extent').get_text()\n",
    "    except:\n",
    "        extent = ''\n",
    "\n",
    "    row = [collection_title, unittitle, unitdate, container_type, container_number, extent]\n",
    "\n",
    "    list_of_lists.append(row)    ## Adding the row list we defined in the previous line to 'list_of_lists' \n",
    "\n",
    "\n",
    "list_of_lists[:15]   ## Outputting the first 15 rows in our list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Almost there! Next we'll run the script above on each XML file in our list, creating a \n",
    "## master list of lists that we'll write to disk as a CSV in the next cell.\n",
    "\n",
    "## Let's begin by re-loading our list of XML filenames:\n",
    "\n",
    "os.chdir('/sharedfolder/LOC_Metadata')\n",
    "xml_filenames = [item for item in os.listdir('./') if item[-2:]=='.2']  # Creating a list of XML filenames\n",
    "\n",
    "list_of_lists = []  # Creating an empty list\n",
    "\n",
    "## Now we'll extract metadata from the full batch of XML files. This may take a few moments to complete.\n",
    "\n",
    "for xml_filename in xml_filenames:\n",
    "    \n",
    "    xml_text = open(xml_filename).read()\n",
    "    \n",
    "    soup = BeautifulSoup(xml_text, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        collection_title = clean_text(soup.find('titleproper').get_text())\n",
    "    except:\n",
    "        collection_title = xml_filename   # If the 'titleproper' field is missing for some reason,\n",
    "                                          ## we'll use the XML filename instead.\n",
    "    \n",
    "    for element in soup.find_all('did'):\n",
    "    \n",
    "        # unittitle\n",
    "        try:\n",
    "            unittitle = clean_text(element.find('unittitle').get_text())\n",
    "        except:\n",
    "            unittitle = ''\n",
    "        \n",
    "        # unitdate\n",
    "        try:\n",
    "            unitdate = clean_text(element.find('unitdate').get_text())\n",
    "        except:\n",
    "            unitdate = ''\n",
    "        \n",
    "        # container type and number\n",
    "        try:\n",
    "            container_type = element.find('container')['type']\n",
    "        except:\n",
    "            container_type = ''\n",
    "    \n",
    "        try:\n",
    "            container_number = element.find('container').get_text()\n",
    "        except:\n",
    "            container_number = ''\n",
    "    \n",
    "        # extent\n",
    "        try:\n",
    "            extent = element.find('extent').get_text()\n",
    "        except:\n",
    "            extent = ''\n",
    "    \n",
    "        row = [collection_title, unittitle, unitdate, container_type, container_number, extent]\n",
    "    \n",
    "        list_of_lists.append(row)\n",
    "\n",
    "\n",
    "print(len(list_of_lists))   ## Printing the number of rows in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, we write the extracted metadata to disk as a CSV called 'LOC_RS_Reduced_Metadata.csv'\n",
    "\n",
    "out_path = \"./LOC_RS_Reduced_Metadata.csv\"  # The './' part is optional; it just means we're writing to \n",
    "                                            # the current working directory.\n",
    "\n",
    "# Defining a list of column headers, which we will write as the first row in our CSV\n",
    "column_headers = ['Collection Title', 'Unit Title', 'Unit Date', 'Container Type', 'Container Number', 'Extent']\n",
    "\n",
    "import csv                               # Importing Python's built-in CSV input/output package\n",
    "\n",
    "with open(out_path, 'w') as fo:          # Creating a tempory file stream object called 'fo' (my abbreviation for 'file out')\n",
    "    csv_writer = csv.writer(fo)          # Initializing our CSV writer\n",
    "    csv_writer.writerow(column_headers)  # Writing one row (our column headers)\n",
    "    csv_writer.writerows(list_of_lists)  # Writing a list of lists as a sequence of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Go to 'sharedfolder' on your desktop and use LibreOffice or Excel to open your new CSV.\n",
    "\n",
    "## As you scroll through the CSV file, you will probably see more formatting oddities you can fix \n",
    "## by tweaking the code above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
